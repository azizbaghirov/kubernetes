Installing a container runtime is not a CKA exam requirement

When we install Kubernetes on a system, we actually install the following components:
On the master nodes:
1. Kube-API Server (responsible for orchestrading all operations within the cluster)
2. etcd (distributed reliable key-value store used by Kubernetes to store all data used to manage the cluster)
3. Scheduler (responsible for distributing work on containers accross multiple nodes, it only decides which pod goes where, it is the job of kubelet)
4. Controller (responsible for noticying and responding when nodes, containers or endpoints go down)
5. Container Runtime (underlying software that is used to run containers, in our case it is Docker)
On the worker nodes:
6. kubelet (agent that is running on all nodes on the cluster, responsible for making sure that containers are running on the nodes as expected)
7. kube-proxy (ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other)

Kubernetes cluster consists of 2 types of nodes: 
● master
● worker

============================ Pods ==============================================

# kubectl get nodes
# kubectl get all
# kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10
# kubectl get deployments
# kubectl expose deployment hello-minikube --type=NodePort --port=8080
# minikube service hello-minikube --url
# kubectl get pods
# kubectl delete pod nginx

Kubernetes does not deploy containers directly on the worker nodes. The containers are encapsulated into the Kubernetes object called PODS. A pod is a single instance of an application. The pods always have one-to-one relationship with its containers, meaning one pod always include just one container. But there might be situations that, alongside with the application inside the pod, the helper container can exist. The pod is the smallest part of the Kubernetes cluster.

# kubectl run nginx --image=nginx
# kubectl describe pod nginx
# kubectl get pods -o wide
# kubectl.exe apply -f c:\Users\User\Desktop\pod.yaml

The column 'READY' in the output of the command 'kubectl get pods' shows us 'Total PODS/Running PODS'

Replication controller helps us run multiple instances of a single pod in the Kubernetes cluster, thus providing high availability. Another reason we need the replication controller is to create multiple pods to share the load accross them. But today, replication controller is replaced by replica set.

============================== Replica Sets =======================================

#kubectl create -f replicaset.yaml
#kubectl get replicaset
If we have specified the number of replicas in the yaml file, then replicaset will try to keep that number of running pods. Even if we delete one pod intentionally, replicaset will create it
# kubectl delete pod myapp-replicaset-j7rzk
# kubectl describe replicaset myapp-replicaset
Unlike replication controller, in the yaml file when creating replicaset we have to specify 'selector' section, because the replicaset also takes care of pods created before the creation of replicaset.
But for this pods must have same label.

Another case, if we try to create a pod from yaml with the same label, replicaset will terminate it.
#kubectl get pods
NAME                     READY   STATUS        RESTARTS   AGE
myapp-replicaset-b4wqh   1/1     Running       0          12m
myapp-replicaset-qskvp   1/1     Running       0          15m
myapp-replicaset-rmfq6   1/1     Running       0          15m
nginx                    0/1     Terminating   0          4s

Get information about cluster components:
# kubectl get componentstatuses

If we want to change the number of replicas:
1. Edit the definition file (change the number of replicas) then:
# kubectl replace -f replicaset-definition.yaml
The first case is advisable in imperative mode, because if we choose the second method, the definition file itself remains unchanged.

2. # kubectl edit replicaset myapp-replicaset

3. # kubectl scale replicaset myapp-replicaset --replicas=5

=============================== Deployments ===========================================

Deployment provides us with the capability to upgrade the underlying instances seemlessly using rolling updates, undo changes and resume changes as required.
# kubectl create -f deployment-helloworld.yaml
# kubectl get deployments
# kubectl describe deployment myapp-deployment

When we first create a deployment it triggers a rollout. A new rollout creates a new deployment revision called revision1. In the future, when the application is upgraded, meaning when the container version is updated to a new one, a new rollout is triggered and new deployment revision is created called revision2. This helps us keep track of changes made to our deployment and enables us to rollback to previous versions if necessary. The default deployment strategy is to update replicas one by one if we dont specify another (rolling update). This process is done with next sequence:
1. When deployment is created, a replicaset is created accordingly with the specified number of pods.
2. When we update the application, a new replicaset is created and pods are replaced one by one.fff
Another option is recreate: it means first destroy all replicas, then deploy newer versions.

Another way of creating deployment is only specifying the image name with below command:
# kubectl run nginx --image=nginx

# kubectl rollout status deployment/myapp-deployment
# kubectl rollout history deployment/myapp-deployment
# kubectl create -f deployment-redis.yaml --record
# kubectl edit deployment myapp-deployment - we can change the version of image by editing opened file.
Changing the version of app in Kubernetes memory.
# kubectl set image deployment myapp-deployment nginx=nginx:1.22-perl --record
To rollout the changes we made in our app:
# kubectl rollout undo deployment myapp-deployment

================================= Namespaces ================================================

When creating kubernetes cluster, by default k8s creates 3 namespaces: 1) default 2) kube-system 3) kube-public
We can also assign resource limits for namespaces. When we create a pod, k8s creates it under default namespace. To create a pod under another namespace, we write:
# kubectl create -f pod-definition.yaml -n dev
Or we can add a line to the definition file, under metadata section: 'namespace: dev'
We can create a namespace by definition file or:
# kubectl create namespace dev
If we want to use dev namespace permanently, without specifying its name, we write:
# kubectl config set-context $(kubectl config current-context) --namespace=dev

In our application, if we want to connect to db in another namespace, we specify the string like this:
mysql.connect("db-service.dev.svc.cluster.local") - where

db-service is the name of service
dev is the name of namespace
svc pointing that the it is a service
cluster.local is the default name assigned to our K8S cluster
================================= Networking ================================================

In a Kubernetes world ip is assigned to pod unlike in docker. Kubernetes requires us to set up network to meet below criterias:
● All containers/pods can communicate to one another without NAT
● All nodes can communicate with all containers and vice-versa without NAT

========================= Services ===========================

Kubernetes service enable communication between various components within and the outside of the application. Kubernetes services help us connect applications together with other apps and users. One of its use case is to listen to a port on the node forward request on that port to a port of the pod running the web app. In this situation there are 3 ports involved:
1. Target port: the port of pod
2. Port (simply): Inside the cluster the service hat its own ip address. This ip is called 'cluster ip of that service'.
3. NodePort: It ranges from 30000 to 32767

# kubectl get services or svc

There are several types of services:
● NopePort
● ClusterIP
● LoadBalancer

Manual Scheduling
If we want to assign a pod manually to a specific node in the spec section we write: nodeName. But we cant change it after creation by editing pod in kubernetes memory. Instead, we can write pod-bind-definition file. 
========================= Labels ===========================

Labels are  properties attached to each item.
We can specify as much labels as we want when creating objects in cluster.
If we want to filter the results when viewing objects, we use selector:
# kubectl get pods --selector app=App1

========================= Taints and Tolerations ===========================

Imagine a situation where we taint a node lets say with 'blue'. And if pods dont have tolerations to this taint, the scheduler will not place this pods to this node. To set taint to a node:
# kubectl taint nodes node2 key=value:taint-effect
In the example above, we see taint-effect, which can have 3 values:
● NoSchedule - means that the scheduler will not place any pod not tolerant on that node
● PreferNoSchedule - the scheduler will try not place any pod not tolerant on that node, but that not guaranteed
● NoExecute - the scheduler will kill pods placed before and not tolerant from this node
Setting a toleration to the pod in definition file, does not guarantee that this pod will placed on the exact node with appropriate taint.

# kubectl taint nodes node2 app=blue:NoSchedule

There are situations when we want to place our pod explicitly to the specified node. For this we use 'node selector' in the pod definition file. But before this, we must label the node with appropriate label.
# kubectl label nodes node-1 size=Large

There are two stages in the lifecycle of the pod when considering the nodeAffinity.
● DuringScheduling - when pod does not exist and created for the 1st time
● DuringExecution

requiredDuringScheduling: The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.
preferredDuringScheduling: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.
IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.
By default, Kubernetes sets a limit of 1 vCPU and 512Mi of memory to each container. The container can not use cpu bigger than specified limit. But the memory can. In that case the pod will be terminated.

========================= Daemon Sets ===========================

DaemonSet ensures that all (or some) nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected.
StaticPods are managed directly by the kubelet daemon on a specific node, without the API server observing them.

# kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml - creates a static pod, places it under manifest dir. and runs immadiately
Tip: /etc/kubernetes/manifest dir. is configured as a static pod dir. in the /var/lib/kubelet/config.yaml file

To view the logs of container within the pod, just like in docker we run:
# kubectl logs -f event-simulator (where -f option is like # tail -f option)
But if there are two or more containers within the pod:
# kubectl logs -f event-simulator image-processor (where image-processor is one of the containers in pod)
